{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfbYBDGMIm5s",
        "outputId": "960f0449-324d-4c5e-b1d3-91144e9be8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "#### upload the modify CSV file to your drive (add features name)\n",
        "os.chdir('/content/gdrive/MyDrive') # change path if needed "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.utils import shuffle\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU37w3JII7vL",
        "outputId": "9bfd24ac-f9d5-40eb-8a57-c7d7e455f0dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "final_test = pd.read_csv('test.csv')\n",
        "train_data = shuffle(train_data)"
      ],
      "metadata": {
        "id": "uWyJb2IyKZug"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Show vectorizer size without any feature construction \n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return ' '.join(tokens)\n",
        "train_data['raw_body'] = train_data['body'].apply(preprocess_text)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(train_data['raw_body'])\n",
        "X.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg1_JA2PLlQT",
        "outputId": "68a20eec-b5d8-424b-8943-53f6c9cd9487"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53906"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "more_stop_words = [\n",
        "        \"docs\",\n",
        "        \"google\",\n",
        "        \"just\",\n",
        "        \"think\",\n",
        "        \"https\",\n",
        "        \"org\",\n",
        "        \"www\",\n",
        "        \"don’t\",\n",
        "        \"like\",\n",
        "        \"need\",\n",
        "        \"it\",\n",
        "        \"you’re\",\n",
        "        \"use\",\n",
        "        \"reddit\",\n",
        "        \"thing\",\n",
        "        \"I’m\",\n",
        "        \"things\",\n",
        "        \"good\",\n",
        "        \"blog\"\n",
        "        \"really\",\n",
        "        \"want\",\n",
        "        \"maybe\",\n",
        "        \"imgur\",\n",
        "        \"com\",\n",
        "        \"don\",\n",
        "        \"actually\",\n",
        "        \"that\",\n",
        "        \"make\",\n",
        "        \"lot\",\n",
        "        \"different\",\n",
        "        \"doing\",\n",
        "        \"that\",\n",
        "        \"better\",\n",
        "        \"going\",\n",
        "        \"great\",\n",
        "        \"fo\",\n",
        "        \"http\",\n",
        "        \"ft\",\n",
        "        \"io\",\n",
        "        \"page\",\n",
        "        \"archive\",\n",
        "        \"web\",\n",
        "        \"ads\",\n",
        "        \"link\",\n",
        "        \"ad\",\n",
        "        \"proxy\",\n",
        "        \"using\",\n",
        "        \"hi\",\n",
        "        \"subreddit\",\n",
        "        \"op\",\n",
        "        \"huh\",\n",
        "        \"would\",\n",
        "        \"html\",\n",
        "        \"cache\",\n",
        "        \"search\",\n",
        "        \"site\",\n",
        "        \"lol\",\n",
        "        \"could\",\n",
        "        \"get\",\n",
        "        \"went\",\n",
        "        \"used\",\n",
        "        \"say\",\n",
        "        \"whenever\",\n",
        "        \"kind\",\n",
        "        \"hello\",\n",
        "        \"bot\",\n",
        "        \"outline\",\n",
        "        \"free\",\n",
        "        \"pdf\",\n",
        "        \"bit\",\n",
        "        \"sure\",\n",
        "        \"made\",\n",
        "        \"make\",\n",
        "        \"something\",\n",
        "        \"long\",\n",
        "        \"name\",\n",
        "        \"take\",\n",
        "        \"whatever\",\n",
        "        \"however\",\n",
        "        \"likely\",\n",
        "        \"co\",\n",
        "        \"hey\",\n",
        "        \"okey\",\n",
        "        \"news\",\n",
        "        \"bullshit\", \n",
        "        \"crap\",\n",
        "        \"newest\",\n",
        "        \"hvtargid\"\n",
        "    ]\n",
        "for letter in string.ascii_lowercase:\n",
        "    more_stop_words.append(letter)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Define a function to remove stop words, punctuations\n",
        "\n",
        "def clean_text(text):\n",
        "    discord_pattern = r\"(?:https?://)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(\\w+)\"\n",
        "    text = re.sub(r'https?://(?:www\\.)?youtube(?:-nocookie)?\\.com/(?:[^/\\n\\s]+/)?(?:watch|embed)(?:\\.php|/|\\?)\\S+', '', text)\n",
        "    text = re.sub(r'https://docs.google.com/viewer\\?url=.+', '', text)\n",
        "    text = re.sub(discord_pattern, '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'/u/\\w+!', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'/r/', '', text)\n",
        "    text = re.sub(r'/sp', '', text)\n",
        "    text = re.sub(r\"'s\", '', text)\n",
        "    text = re.sub(r'_[a-zA-Z0-9]_[a-zA-Z0-9]+', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chars_to_remove = stop_words\n",
        "    chars_to_remove.update(more_stop_words)\n",
        "    tokens = [token for token in tokens if token not in chars_to_remove]\n",
        "    filtered_tokens = [token for token in tokens if len(token) > 2]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "XFGZdEnHQAEz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bernoulli Naïve Bayes from scratch\n",
        "class myBernoulliNB:\n",
        "    def __init__(self, alpha=1.0, binarize=0.0):\n",
        "        self.alpha = alpha\n",
        "        self.binarize = binarize\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes_ = np.unique(y)\n",
        "        n_classes = len(self.classes_)\n",
        "        self.class_count_ = np.zeros(n_classes, dtype=int)\n",
        "        self.feature_count_ = np.zeros((n_classes, n_features), dtype=float)\n",
        "        self.class_log_prior_ = np.zeros(n_classes, dtype=float)\n",
        "        self.alpha *= n_classes\n",
        "\n",
        "        # Compute class count and feature count\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            X_c = X[y == c]\n",
        "            self.class_count_[i] = X_c.shape[0]\n",
        "            self.feature_count_[i] = (X_c > self.binarize).sum(axis=0)\n",
        "\n",
        "        # Compute class log prior\n",
        "        n_samples = y.shape[0]\n",
        "        self.class_log_prior_ = np.log(self.class_count_ + self.alpha) - np.log(n_samples + self.alpha * n_classes)\n",
        "\n",
        "        # Compute feature log probability\n",
        "        self.feature_log_prob_ = (np.log(self.feature_count_ + self.alpha)\n",
        "                                  - np.log(self.class_count_[:, np.newaxis] + self.alpha * 2))\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = X.toarray()\n",
        "        # Compute log likelihoods\n",
        "        neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
        "        log_prob = np.dot(X, self.feature_log_prob_.T) + np.dot(1 - X, neg_prob.T) + self.class_log_prior_\n",
        "\n",
        "        # Return class with highest log likelihood\n",
        "        y_pred = self.classes_[np.argmax(log_prob, axis=1)]\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        # Compute log likelihoods\n",
        "        neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
        "        log_prob = np.dot(X, self.feature_log_prob_.T) + np.dot(1 - X, neg_prob.T) + self.class_log_prior_\n",
        "\n",
        "        # Convert log probabilities to probabilities\n",
        "        prob = np.exp(log_prob - log_prob.max(axis=1)[:, np.newaxis])\n",
        "        prob /= prob.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        return prob\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        acc = np.mean(y_pred == y)\n",
        "        return acc\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "\n",
        "        params = {\"alpha\": self.alpha,\n",
        "        \"binarize\": self.binarize}\n",
        "        return params\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "          setattr(self, parameter, value)\n",
        "        return self\n",
        "        "
      ],
      "metadata": {
        "id": "_vH9Auf_NRlq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean_text() function to the 'body' column of the train_data dataframe\n",
        "train_data['clean_text'] = train_data['body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "MOsRsC_cQGmL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "-3lfy4DUQL6j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words and URL filtering"
      ],
      "metadata": {
        "id": "kmw104Efrrie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgDFhJWEyhoV",
        "outputId": "22d067d6-7dda-478f-c4ba-b11c84cc60ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9470765345765345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ABQEAeqyyF-",
        "outputId": "092a2873-8a05-4466-dcc5-7227a6449202"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9192016317016318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
        "              'tfidf__use_idf': [True, False],\n",
        "              'clf-svm__C': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "              'clf-svm__kernel': ['rbf']}\n",
        "text_model_svm2 = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SVC(max_iter=1000, random_state=42))])  \n",
        "grid_search = GridSearchCV(text_model_svm2, param_grid, cv=5)\n",
        "\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1nubCQ1vRb7",
        "outputId": "27009629-4354-4ac2-e09a-51b2989a0efd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:  {'clf-svm__C': 2.0, 'clf-svm__kernel': 'rbf', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.8997086247086248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBCiAIDNt5FW",
        "outputId": "daab0c02-e8fe-44e6-db8b-88cacbe2380e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters:  {'clf__alpha': 0.5, 'vect__ngram_range': (1, 3)}\n",
            "Best score:  0.9233682983682984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('clf', myBernoulliNB())])\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "              'clf__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "              'clf__binarize': [1]}\n",
        "\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CWvrnmIaGwa",
        "outputId": "e7e4e830-25bc-4780-8b78-b6a3f67c3a0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "Best hyperparameters: {'clf__alpha': 0.1, 'clf__binarize': 1, 'vect__ngram_range': (1, 1)}\n",
            "Accuracy: 0.8913364413364414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization "
      ],
      "metadata": {
        "id": "rsnpCr2Soak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text2(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in lemmatized_tokens if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "oZEfAYqLog_z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean_text() function to the 'body' column of the train_data dataframe\n",
        "train_data['clean_text2'] = train_data['body'].apply(clean_text2)\n",
        "X = train_data['clean_text2']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "e7OlMys6sLGY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZqDwhisy5wf",
        "outputId": "9730f6ec-7268-452d-84d3-f88b097ab8b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9387043512043511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7AZFGZay4dn",
        "outputId": "59785487-88ff-4c44-e8ef-03e5926027a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9066822066822067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
        "              'tfidf__use_idf': [True, False],\n",
        "              'clf-svm__C': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "              'clf-svm__kernel': ['rbf']}\n",
        "text_model_svm2 = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SVC(max_iter=1000, random_state=42))])  \n",
        "grid_search = GridSearchCV(text_model_svm2, param_grid, cv=5)\n",
        "\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT3wSHr-vf4T",
        "outputId": "51b130d9-5c13-4ecf-8f95-28b83a4e188c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:  {'clf-svm__C': 2.0, 'clf-svm__kernel': 'rbf', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.8983391608391609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd0Tku8kt8v2",
        "outputId": "200b34f0-e40d-4ba8-eae5-369cf5b27c84"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters:  {'clf__alpha': 0.1, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9164432789432789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('clf', myBernoulliNB())])\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "              'clf__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "              'clf__binarize': [1]}\n",
        "\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R71YXMDmsQPo",
        "outputId": "b52f5d9b-6bd8-41ff-b58b-960f331f55b1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "Best hyperparameters: {'clf__alpha': 0.01, 'clf__binarize': 1, 'vect__ngram_range': (1, 3)}\n",
            "Accuracy: 0.8370920745920747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "L6yId1ZKoheT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text3(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in stemmed_tokens if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "3p5lnGHJomeM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean_text() function to the 'body' column of the train_data dataframe\n",
        "train_data['clean_text3'] = train_data['body'].apply(clean_text3)\n",
        "X = train_data['clean_text3']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "NZnXhSdhsaW5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxZEbDPiy95P",
        "outputId": "08f1ce7c-a6b2-4331-a566-1a8066fc1399"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9373251748251749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1bN6SCjy_OG",
        "outputId": "6acce584-0084-45bf-d0fc-e300f0f67d92"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9094794094794094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
        "              'tfidf__use_idf': [True, False],\n",
        "              'clf-svm__C': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "              'clf-svm__kernel': ['rbf']}\n",
        "text_model_svm2 = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SVC(max_iter=1000, random_state=42))])  \n",
        "grid_search = GridSearchCV(text_model_svm2, param_grid, cv=5)\n",
        "\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WawMdamvhdj",
        "outputId": "0846888f-4191-477c-8d87-5e6734522d3d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:  {'clf-svm__C': 2.0, 'clf-svm__kernel': 'rbf', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9052836052836053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pzZ6ZC4uA9v",
        "outputId": "c83e1e64-1e2b-480a-d288-15c1171b96f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters:  {'clf__alpha': 0.1, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9220376845376844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('clf', myBernoulliNB())])\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "              'clf__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "              'clf__binarize': [1]}\n",
        "\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dixtgr4Lsznb",
        "outputId": "31b18a78-0dcc-4c97-f4e2-e3070f2911c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "Best hyperparameters: {'clf__alpha': 0.1, 'clf__binarize': 1, 'vect__ngram_range': (1, 2)}\n",
            "Accuracy: 0.8440462315462316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging"
      ],
      "metadata": {
        "id": "Q5qr1RFWom60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text4(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    filtered_tokens2 = [token for token, pos in tagged_tokens if pos.startswith('N') or pos.startswith('J') or pos.startswith('V')]\n",
        "    filtered_tokens = [token for token in filtered_tokens2 if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "NF9y71PAoyDU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean_text() function to the 'body' column of the train_data dataframe\n",
        "train_data['clean_text4'] = train_data['body'].apply(clean_text4)\n",
        "X = train_data['clean_text4']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "H_M6WHrtsdrh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxhiYp12zFIf",
        "outputId": "bcd49d81-a470-446f-97bf-47508715569e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.941501554001554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SGDClassifier(max_iter=1000, random_state=42))])\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf-svm__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7jwh38UzGBv",
        "outputId": "b3a52380-6112-4054-b2f6-5d9c87fe0f89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.001, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.909450271950272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
        "              'tfidf__use_idf': [True, False],\n",
        "              'clf-svm__C': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "              'clf-svm__kernel': ['rbf']}\n",
        "text_model_svm2 = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf-svm', SVC(max_iter=1000, random_state=42))])  \n",
        "grid_search = GridSearchCV(text_model_svm2, param_grid, cv=5)\n",
        "\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnhZKk4WwKrl",
        "outputId": "f2141f4f-1142-4aa5-8d4a-7373ff35338d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:  {'clf-svm__C': 2.0, 'clf-svm__kernel': 'rbf', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.8997280497280495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'clf__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10],\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9xckqBSuCav",
        "outputId": "ed2d4c52-8b9d-441d-a258-c78fae751ae9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best parameters:  {'clf__alpha': 0.5, 'vect__ngram_range': (1, 3)}\n",
            "Best score:  0.9206196581196581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('clf', myBernoulliNB())])\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "              'clf__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
        "              'clf__binarize': [1]}\n",
        "\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfRVDIJHs0_a",
        "outputId": "7d424443-36a0-495c-98d3-3ae16f62a84a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
            "Best hyperparameters: {'clf__alpha': 0.1, 'clf__binarize': 1, 'vect__ngram_range': (1, 1)}\n",
            "Accuracy: 0.8969502719502719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Model and Feature Selection"
      ],
      "metadata": {
        "id": "KsP9Q0_voyz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data again\n",
        "train_data = pd.read_csv('train.csv')\n",
        "final_test = pd.read_csv('test.csv')\n",
        "train_data = shuffle(train_data)\n"
      ],
      "metadata": {
        "id": "Eut_Ru4ho308"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stop words an URL filtering"
      ],
      "metadata": {
        "id": "2Q3oiUwE9-2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "more_stop_words = [\n",
        "        \"docs\",\n",
        "        \"google\",\n",
        "        \"just\",\n",
        "        \"think\",\n",
        "        \"https\",\n",
        "        \"org\",\n",
        "        \"www\",\n",
        "        \"don’t\",\n",
        "        \"like\",\n",
        "        \"need\",\n",
        "        \"it\",\n",
        "        \"you’re\",\n",
        "        \"use\",\n",
        "        \"reddit\",\n",
        "        \"thing\",\n",
        "        \"I’m\",\n",
        "        \"things\",\n",
        "        \"good\",\n",
        "        \"blog\"\n",
        "        \"really\",\n",
        "        \"want\",\n",
        "        \"maybe\",\n",
        "        \"imgur\",\n",
        "        \"com\",\n",
        "        \"don\",\n",
        "        \"actually\",\n",
        "        \"that\",\n",
        "        \"make\",\n",
        "        \"lot\",\n",
        "        \"different\",\n",
        "        \"doing\",\n",
        "        \"that\",\n",
        "        \"better\",\n",
        "        \"going\",\n",
        "        \"great\",\n",
        "        \"fo\",\n",
        "        \"http\",\n",
        "        \"ft\",\n",
        "        \"io\",\n",
        "        \"page\",\n",
        "        \"archive\",\n",
        "        \"web\",\n",
        "        \"ads\",\n",
        "        \"link\",\n",
        "        \"ad\",\n",
        "        \"proxy\",\n",
        "        \"using\",\n",
        "        \"hi\",\n",
        "        \"subreddit\",\n",
        "        \"op\",\n",
        "        \"huh\",\n",
        "        \"would\",\n",
        "        \"html\",\n",
        "        \"cache\",\n",
        "        \"search\",\n",
        "        \"site\",\n",
        "        \"lol\",\n",
        "        \"could\",\n",
        "        \"get\",\n",
        "        \"went\",\n",
        "        \"used\",\n",
        "        \"say\",\n",
        "        \"whenever\",\n",
        "        \"kind\",\n",
        "        \"hello\",\n",
        "        \"bot\",\n",
        "        \"outline\",\n",
        "        \"free\",\n",
        "        \"pdf\",\n",
        "        \"bit\",\n",
        "        \"sure\",\n",
        "        \"made\",\n",
        "        \"make\",\n",
        "        \"something\",\n",
        "        \"long\",\n",
        "        \"name\",\n",
        "        \"take\",\n",
        "        \"whatever\",\n",
        "        \"however\",\n",
        "        \"likely\",\n",
        "        \"co\",\n",
        "        \"hey\",\n",
        "        \"okey\",\n",
        "        \"news\",\n",
        "        \"bullshit\", \n",
        "        \"crap\",\n",
        "        \"newest\",\n",
        "        \"hvtargid\"\n",
        "    ]\n",
        "for letter in string.ascii_lowercase:\n",
        "    more_stop_words.append(letter)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Define a function to remove stop words, punctuations\n",
        "\n",
        "def clean_text(text):\n",
        "    discord_pattern = r\"(?:https?://)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(\\w+)\"\n",
        "    text = re.sub(r'https?://(?:www\\.)?youtube(?:-nocookie)?\\.com/(?:[^/\\n\\s]+/)?(?:watch|embed)(?:\\.php|/|\\?)\\S+', '', text)\n",
        "    text = re.sub(r'https://docs.google.com/viewer\\?url=.+', '', text)\n",
        "    text = re.sub(discord_pattern, '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'/u/\\w+!', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'/r/', '', text)\n",
        "    text = re.sub(r'/sp', '', text)\n",
        "    text = re.sub(r\"'s\", '', text)\n",
        "    text = re.sub(r'_[a-zA-Z0-9]_[a-zA-Z0-9]+', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chars_to_remove = stop_words\n",
        "    chars_to_remove.update(more_stop_words)\n",
        "    tokens = [token for token in tokens if token not in chars_to_remove]\n",
        "    filtered_tokens = [token for token in tokens if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "9mVhjBnFycxF"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the clean_text() function to the 'body' column of the train_data dataframe\n",
        "train_data['clean_text'] = train_data['body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "64pm8GCNyxsn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subreddit in ['Obama', 'Musk', 'Trump', 'Ford']:\n",
        "    unwanted_subs = ['elon','musk', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Obama' else \\\n",
        "                    ['barack', 'obama', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Musk' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'ford', 'joe', 'biden'] if subreddit == 'Trump' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'donald', 'trump', 'joe', 'biden']\n",
        "    train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'] = train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in unwanted_subs]))\n",
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "9-isurPc0L9f"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "w51LLYHa0RTk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the text data\n",
        "X_new = vectorizer.fit_transform(X)\n",
        "\n",
        "# Create a SelectKBest object to select the k best features\n",
        "selecter = SelectKBest(chi2, k=10)\n",
        "\n",
        "# Transform the text data to select the k best features\n",
        "X_new = selecter.fit_transform(X_new, y)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = np.array(list(vectorizer.vocabulary_.keys()))\n",
        "sorted_indices = np.argsort(list(vectorizer.vocabulary_.values()))\n",
        "sorted_features = feature_names[sorted_indices]\n",
        "\n",
        "# Print the 10 most important words\n",
        "selected_words = pd.DataFrame(data={\"words\" :sorted_features[selecter.get_support()]})\n",
        "selected_words[\"scores\"] = selecter.scores_[selecter.get_support()]\n",
        "df_sorted = selected_words.sort_values(by='scores', ascending=False)\n",
        "df_sorted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "YlQNWh94zTUp",
        "outputId": "05746459-71cd-41f9-d5f4-4f71caa024d2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     words       scores\n",
              "4    obama  2728.855556\n",
              "8    tesla  1653.618896\n",
              "0   barack  1010.244444\n",
              "9    trump  1004.266667\n",
              "6   please   550.031504\n",
              "5  paywall   496.155556\n",
              "2   energy   482.847963\n",
              "3     full   461.562997\n",
              "1   behind   444.390367\n",
              "7    power   436.874835"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f6300d5c-e072-46fc-9ab7-586e3fb22a89\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>obama</td>\n",
              "      <td>2728.855556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tesla</td>\n",
              "      <td>1653.618896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>barack</td>\n",
              "      <td>1010.244444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>trump</td>\n",
              "      <td>1004.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>please</td>\n",
              "      <td>550.031504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>paywall</td>\n",
              "      <td>496.155556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>energy</td>\n",
              "      <td>482.847963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>full</td>\n",
              "      <td>461.562997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>behind</td>\n",
              "      <td>444.390367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>power</td>\n",
              "      <td>436.874835</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6300d5c-e072-46fc-9ab7-586e3fb22a89')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f6300d5c-e072-46fc-9ab7-586e3fb22a89 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f6300d5c-e072-46fc-9ab7-586e3fb22a89');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of features:\", len(feature_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqqptmg00C0b",
        "outputId": "0ac77461-b0ee-4482-91f8-3108d1654ba2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 7602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('select', SelectKBest(chi2)),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 5500],\n",
        "    'clf__alpha': [0.5, 0.8, 1.0, 2.0, 4.0],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RscuthIE4J6r",
        "outputId": "f8af6d67-bc4e-4789-993f-608d38ab8477"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
            "Best parameters:  {'clf__alpha': 0.5, 'select__k': 5500, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9345765345765346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyT0kwbR4Voa",
        "outputId": "cf03c04c-7fae-4020-88ac-8b3148ce48f7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 1.5e-05, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'elasticnet', 'select__k': 'all', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}\n",
            "Best score:  0.9624514374514375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKrfCEhx40c8",
        "outputId": "a4e8a357-2305-4fba-8b94-79a399f3908c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 1.5e-06, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l1', 'select__k': 'all', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9624417249417251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lemmatized"
      ],
      "metadata": {
        "id": "GV9hFjJt961O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Define a function to remove stop words and punctuations\n",
        "\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "more_stop_words = [\n",
        "        \"docs\",\n",
        "        \"google\",\n",
        "        \"just\",\n",
        "        \"think\",\n",
        "        \"https\",\n",
        "        \"org\",\n",
        "        \"www\",\n",
        "        \"don’t\",\n",
        "        \"like\",\n",
        "        \"need\",\n",
        "        \"it\",\n",
        "        \"you’re\",\n",
        "        \"use\",\n",
        "        \"reddit\",\n",
        "        \"thing\",\n",
        "        \"I’m\",\n",
        "        \"things\",\n",
        "        \"good\",\n",
        "        \"blog\"\n",
        "        \"really\",\n",
        "        \"want\",\n",
        "        \"maybe\",\n",
        "        \"imgur\",\n",
        "        \"com\",\n",
        "        \"don\",\n",
        "        \"actually\",\n",
        "        \"that\",\n",
        "        \"make\",\n",
        "        \"lot\",\n",
        "        \"different\",\n",
        "        \"doing\",\n",
        "        \"that\",\n",
        "        \"better\",\n",
        "        \"going\",\n",
        "        \"great\",\n",
        "        \"fo\",\n",
        "        \"http\",\n",
        "        \"ft\",\n",
        "        \"io\",\n",
        "        \"page\",\n",
        "        \"archive\",\n",
        "        \"web\",\n",
        "        \"ads\",\n",
        "        \"link\",\n",
        "        \"ad\",\n",
        "        \"proxy\",\n",
        "        \"using\",\n",
        "        \"hi\",\n",
        "        \"subreddit\",\n",
        "        \"op\",\n",
        "        \"huh\",\n",
        "        \"would\",\n",
        "        \"html\",\n",
        "        \"cache\",\n",
        "        \"search\",\n",
        "        \"site\",\n",
        "        \"lol\",\n",
        "        \"could\",\n",
        "        \"get\",\n",
        "        \"went\",\n",
        "        \"used\",\n",
        "        \"say\",\n",
        "        \"whenever\",\n",
        "        \"kind\",\n",
        "        \"hello\",\n",
        "        \"bot\",\n",
        "        \"outline\",\n",
        "        \"free\",\n",
        "        \"pdf\",\n",
        "        \"bit\",\n",
        "        \"sure\",\n",
        "        \"made\",\n",
        "        \"make\",\n",
        "        \"something\",\n",
        "        \"long\",\n",
        "        \"name\",\n",
        "        \"take\",\n",
        "        \"whatever\",\n",
        "        \"however\",\n",
        "        \"likely\",\n",
        "        \"co\",\n",
        "        \"hey\",\n",
        "        \"okey\",\n",
        "        \"news\",\n",
        "        \"bullshit\", \n",
        "        \"crap\",\n",
        "        \"newest\",\n",
        "        \"hvtargid\"\n",
        "    ]\n",
        "for letter in string.ascii_lowercase:\n",
        "    more_stop_words.append(letter)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Define a function to remove stop words, punctuations\n",
        "\n",
        "def clean_text(text):\n",
        "    discord_pattern = r\"(?:https?://)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(\\w+)\"\n",
        "    text = re.sub(r'https?://(?:www\\.)?youtube(?:-nocookie)?\\.com/(?:[^/\\n\\s]+/)?(?:watch|embed)(?:\\.php|/|\\?)\\S+', '', text)\n",
        "    text = re.sub(r'https://docs.google.com/viewer\\?url=.+', '', text)\n",
        "    text = re.sub(discord_pattern, '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'/u/\\w+!', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'/r/', '', text)\n",
        "    text = re.sub(r'/sp', '', text)\n",
        "    text = re.sub(r\"'s\", '', text)\n",
        "    text = re.sub(r'_[a-zA-Z0-9]_[a-zA-Z0-9]+', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chars_to_remove = stop_words\n",
        "    chars_to_remove.update(more_stop_words)\n",
        "    tokens = [token for token in tokens if token not in chars_to_remove]\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    #filtered_tokens2 = [token for token, pos in tagged_tokens if pos.startswith('N') or pos.startswith('J') or pos.startswith('V')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in lemmatized_tokens if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "bJtYFQP_6fiy"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data again\n",
        "train_data = pd.read_csv('train.csv')\n",
        "final_test = pd.read_csv('test.csv')\n",
        "train_data = shuffle(train_data)"
      ],
      "metadata": {
        "id": "f39vcAvy6lIa"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['clean_text'] = train_data['body'].apply(clean_text)\n",
        "for subreddit in ['Obama', 'Musk', 'Trump', 'Ford']:\n",
        "    unwanted_subs = ['elon','musk', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Obama' else \\\n",
        "                    ['barack', 'obama', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Musk' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'ford', 'joe', 'biden'] if subreddit == 'Trump' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'donald', 'trump', 'joe', 'biden']\n",
        "    train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'] = train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in unwanted_subs]))\n",
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "GLBG8pxR6tNz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('select', SelectKBest(chi2)),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 5500],\n",
        "    'clf__alpha': [0.5, 0.8, 1.0, 2.0, 4.0],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e218faba-4a00-4b86-9b8b-939aab6f8810",
        "id": "qx3qYy1b69Ik"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
            "Best parameters:  {'clf__alpha': 0.5, 'select__k': 5000, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.934508547008547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60b94e5-579a-4268-cf2a-0267747f1b17",
        "id": "-mBw2sAI69Il"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.00015, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'elasticnet', 'select__k': 5000, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9637626262626263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc1847f-a1d1-43f2-b841-f04216b81f50",
        "id": "YmaXiVkN69Il"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 1.5e-06, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l1', 'select__k': 5000, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}\n",
            "Best score:  0.9665695415695416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pos tagging"
      ],
      "metadata": {
        "id": "ZIgfTeUS-F1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Define a function to remove stop words and punctuations\n",
        "\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "more_stop_words = [\n",
        "        \"docs\",\n",
        "        \"google\",\n",
        "        \"just\",\n",
        "        \"think\",\n",
        "        \"https\",\n",
        "        \"org\",\n",
        "        \"www\",\n",
        "        \"don’t\",\n",
        "        \"like\",\n",
        "        \"need\",\n",
        "        \"it\",\n",
        "        \"you’re\",\n",
        "        \"use\",\n",
        "        \"reddit\",\n",
        "        \"thing\",\n",
        "        \"I’m\",\n",
        "        \"things\",\n",
        "        \"good\",\n",
        "        \"blog\"\n",
        "        \"really\",\n",
        "        \"want\",\n",
        "        \"maybe\",\n",
        "        \"imgur\",\n",
        "        \"com\",\n",
        "        \"don\",\n",
        "        \"actually\",\n",
        "        \"that\",\n",
        "        \"make\",\n",
        "        \"lot\",\n",
        "        \"different\",\n",
        "        \"doing\",\n",
        "        \"that\",\n",
        "        \"better\",\n",
        "        \"going\",\n",
        "        \"great\",\n",
        "        \"fo\",\n",
        "        \"http\",\n",
        "        \"ft\",\n",
        "        \"io\",\n",
        "        \"page\",\n",
        "        \"archive\",\n",
        "        \"web\",\n",
        "        \"ads\",\n",
        "        \"link\",\n",
        "        \"ad\",\n",
        "        \"proxy\",\n",
        "        \"using\",\n",
        "        \"hi\",\n",
        "        \"subreddit\",\n",
        "        \"op\",\n",
        "        \"huh\",\n",
        "        \"would\",\n",
        "        \"html\",\n",
        "        \"cache\",\n",
        "        \"search\",\n",
        "        \"site\",\n",
        "        \"lol\",\n",
        "        \"could\",\n",
        "        \"get\",\n",
        "        \"went\",\n",
        "        \"used\",\n",
        "        \"say\",\n",
        "        \"whenever\",\n",
        "        \"kind\",\n",
        "        \"hello\",\n",
        "        \"bot\",\n",
        "        \"outline\",\n",
        "        \"free\",\n",
        "        \"pdf\",\n",
        "        \"bit\",\n",
        "        \"sure\",\n",
        "        \"made\",\n",
        "        \"make\",\n",
        "        \"something\",\n",
        "        \"long\",\n",
        "        \"name\",\n",
        "        \"take\",\n",
        "        \"whatever\",\n",
        "        \"however\",\n",
        "        \"likely\",\n",
        "        \"co\",\n",
        "        \"hey\",\n",
        "        \"okey\",\n",
        "        \"news\",\n",
        "        \"bullshit\", \n",
        "        \"crap\",\n",
        "        \"newest\",\n",
        "        \"hvtargid\"\n",
        "    ]\n",
        "for letter in string.ascii_lowercase:\n",
        "    more_stop_words.append(letter)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Define a function to remove stop words, punctuations\n",
        "\n",
        "def clean_text(text):\n",
        "    discord_pattern = r\"(?:https?://)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(\\w+)\"\n",
        "    text = re.sub(r'https?://(?:www\\.)?youtube(?:-nocookie)?\\.com/(?:[^/\\n\\s]+/)?(?:watch|embed)(?:\\.php|/|\\?)\\S+', '', text)\n",
        "    text = re.sub(r'https://docs.google.com/viewer\\?url=.+', '', text)\n",
        "    text = re.sub(discord_pattern, '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'/u/\\w+!', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'/r/', '', text)\n",
        "    text = re.sub(r'/sp', '', text)\n",
        "    text = re.sub(r\"'s\", '', text)\n",
        "    text = re.sub(r'_[a-zA-Z0-9]_[a-zA-Z0-9]+', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chars_to_remove = stop_words\n",
        "    chars_to_remove.update(more_stop_words)\n",
        "    tokens = [token for token in tokens if token not in chars_to_remove]\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    filtered_tokens2 = [token for token, pos in tagged_tokens if pos.startswith('N') or pos.startswith('J') or pos.startswith('V')]\n",
        "    #lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in filtered_tokens2 if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "cZPvY5sl7GNw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data again\n",
        "train_data = pd.read_csv('train.csv')\n",
        "final_test = pd.read_csv('test.csv')\n",
        "train_data = shuffle(train_data)"
      ],
      "metadata": {
        "id": "FtP3AUbO7GNw"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['clean_text'] = train_data['body'].apply(clean_text)\n",
        "for subreddit in ['Obama', 'Musk', 'Trump', 'Ford']:\n",
        "    unwanted_subs = ['elon','musk', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Obama' else \\\n",
        "                    ['barack', 'obama', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Musk' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'ford', 'joe', 'biden'] if subreddit == 'Trump' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'donald', 'trump', 'joe', 'biden']\n",
        "    train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'] = train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in unwanted_subs]))\n",
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "ErkVS-XX7GNw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('select', SelectKBest(chi2)),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 5500],\n",
        "    'clf__alpha': [0.5, 0.8, 1.0, 2.0, 4.0],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaad74c1-9904-4e73-a051-b5ec499b81d7",
        "id": "pjdh0cmb7f2X"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
            "Best parameters:  {'clf__alpha': 1.0, 'select__k': 3000, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9317210567210568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cb3e99-3450-43d9-ed50-4796fae1602e",
        "id": "E8XOZAQF7f2X"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.00015, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'select__k': 'all', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9595376845376846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 5000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fd269f-3a9c-4367-c8b0-6c569f0e5a77",
        "id": "fOkUIagf7f2X"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.00015, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'select__k': 'all', 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9540015540015541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemmed"
      ],
      "metadata": {
        "id": "iSeeB8cB-RPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stop words\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Define a function to remove stop words and punctuations\n",
        "\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "more_stop_words = [\n",
        "        \"docs\",\n",
        "        \"google\",\n",
        "        \"just\",\n",
        "        \"think\",\n",
        "        \"https\",\n",
        "        \"org\",\n",
        "        \"www\",\n",
        "        \"don’t\",\n",
        "        \"like\",\n",
        "        \"need\",\n",
        "        \"it\",\n",
        "        \"you’re\",\n",
        "        \"use\",\n",
        "        \"reddit\",\n",
        "        \"thing\",\n",
        "        \"I’m\",\n",
        "        \"things\",\n",
        "        \"good\",\n",
        "        \"blog\"\n",
        "        \"really\",\n",
        "        \"want\",\n",
        "        \"maybe\",\n",
        "        \"imgur\",\n",
        "        \"com\",\n",
        "        \"don\",\n",
        "        \"actually\",\n",
        "        \"that\",\n",
        "        \"make\",\n",
        "        \"lot\",\n",
        "        \"different\",\n",
        "        \"doing\",\n",
        "        \"that\",\n",
        "        \"better\",\n",
        "        \"going\",\n",
        "        \"great\",\n",
        "        \"fo\",\n",
        "        \"http\",\n",
        "        \"ft\",\n",
        "        \"io\",\n",
        "        \"page\",\n",
        "        \"archive\",\n",
        "        \"web\",\n",
        "        \"ads\",\n",
        "        \"link\",\n",
        "        \"ad\",\n",
        "        \"proxy\",\n",
        "        \"using\",\n",
        "        \"hi\",\n",
        "        \"subreddit\",\n",
        "        \"op\",\n",
        "        \"huh\",\n",
        "        \"would\",\n",
        "        \"html\",\n",
        "        \"cache\",\n",
        "        \"search\",\n",
        "        \"site\",\n",
        "        \"lol\",\n",
        "        \"could\",\n",
        "        \"get\",\n",
        "        \"went\",\n",
        "        \"used\",\n",
        "        \"say\",\n",
        "        \"whenever\",\n",
        "        \"kind\",\n",
        "        \"hello\",\n",
        "        \"bot\",\n",
        "        \"outline\",\n",
        "        \"free\",\n",
        "        \"pdf\",\n",
        "        \"bit\",\n",
        "        \"sure\",\n",
        "        \"made\",\n",
        "        \"make\",\n",
        "        \"something\",\n",
        "        \"long\",\n",
        "        \"name\",\n",
        "        \"take\",\n",
        "        \"whatever\",\n",
        "        \"however\",\n",
        "        \"likely\",\n",
        "        \"co\",\n",
        "        \"hey\",\n",
        "        \"okey\",\n",
        "        \"news\",\n",
        "        \"bullshit\", \n",
        "        \"crap\",\n",
        "        \"newest\",\n",
        "        \"hvtargid\"\n",
        "    ]\n",
        "for letter in string.ascii_lowercase:\n",
        "    more_stop_words.append(letter)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "# Define a function to remove stop words, punctuations\n",
        "\n",
        "def clean_text(text):\n",
        "    discord_pattern = r\"(?:https?://)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(\\w+)\"\n",
        "    text = re.sub(r'https?://(?:www\\.)?youtube(?:-nocookie)?\\.com/(?:[^/\\n\\s]+/)?(?:watch|embed)(?:\\.php|/|\\?)\\S+', '', text)\n",
        "    text = re.sub(r'https://docs.google.com/viewer\\?url=.+', '', text)\n",
        "    text = re.sub(discord_pattern, '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'/u/\\w+!', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'/r/', '', text)\n",
        "    text = re.sub(r'/sp', '', text)\n",
        "    text = re.sub(r\"'s\", '', text)\n",
        "    text = re.sub(r'_[a-zA-Z0-9]_[a-zA-Z0-9]+', '', text)\n",
        "    text = re.sub(r'_', '', text)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chars_to_remove = stop_words\n",
        "    chars_to_remove.update(more_stop_words)\n",
        "    tokens = [token for token in tokens if token not in chars_to_remove]\n",
        "    #tagged_tokens = pos_tag(tokens)\n",
        "    #filtered_tokens2 = [token for token, pos in tagged_tokens if pos.startswith('N') or pos.startswith('J') or pos.startswith('V')]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    filtered_tokens = [token for token in stemmed_tokens if len(token) > 3]\n",
        "    return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "nsJLyU2g8Xcm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data again\n",
        "train_data = pd.read_csv('train.csv')\n",
        "final_test = pd.read_csv('test.csv')\n",
        "train_data = shuffle(train_data)"
      ],
      "metadata": {
        "id": "nFc3o3HI8Xcm"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['clean_text'] = train_data['body'].apply(clean_text)\n",
        "for subreddit in ['Obama', 'Musk', 'Trump', 'Ford']:\n",
        "    unwanted_subs = ['elon','musk', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Obama' else \\\n",
        "                    ['barack', 'obama', 'donald', 'trump', 'ford', 'joe', 'biden'] if subreddit == 'Musk' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'ford', 'joe', 'biden'] if subreddit == 'Trump' else \\\n",
        "                    ['barack', 'obama','elon', 'musk', 'donald', 'trump', 'joe', 'biden']\n",
        "    train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'] = train_data.loc[train_data['subreddit'] == subreddit, 'clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in unwanted_subs]))\n",
        "X = train_data['clean_text']\n",
        "y = train_data['subreddit'].replace({'Ford': 1, 'Musk': 2, 'Obama': 3, 'Trump':4})"
      ],
      "metadata": {
        "id": "XgIfVz_K8Xcn"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = Pipeline([('vect', CountVectorizer()),\n",
        "                       ('normalize', Normalizer()),\n",
        "                       ('select', SelectKBest(chi2)),\n",
        "                       ('tfidf', TfidfTransformer()),\n",
        "                       ('clf', MultinomialNB())])\n",
        "\n",
        "# define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1, 3)],\n",
        "    'select__k': [1000, 3000, 'all'],\n",
        "    'clf__alpha': [0.5, 0.8, 1.0, 2.0, 4.0],\n",
        "    'tfidf__use_idf': (True, False)\n",
        "}\n",
        "\n",
        "\n",
        "# perform the grid search\n",
        "grid_search = GridSearchCV(text_model, param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aadb799-2ffa-4554-8445-f09ce9e786c5",
        "id": "2vrrP_VE8PQr"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
            "Best parameters:  {'clf__alpha': 2.0, 'select__k': 1000, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
            "Best score:  0.9358780108780109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['hinge'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4dfa8d-3238-49bf-b31d-ca42d09ff420",
        "id": "0OHMCxME8PQr"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "Best parameters:  {'clf-svm__alpha': 0.0015, 'clf-svm__loss': 'hinge', 'clf-svm__penalty': 'l2', 'select__k': 3000, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n",
            "Best score:  0.9637432012432013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(chi2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf-svm', SGDClassifier(max_iter=5000))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'select__k': [1000, 3000, 'all'],\n",
        "    'clf-svm__alpha': [1.5e-3, 1.5e-4, 1.5e-5, 1.5e-2, 1.5e-6, 1.5e-7],\n",
        "    'clf-svm__loss': ['log_loss'],\n",
        "    'clf-svm__penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(text_model_svm, parameters, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d78155-c509-4f3a-fa15-8c3773ab2003",
        "id": "Cz7n-2YT8PQr"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "Best parameters:  {'clf-svm__alpha': 1.5e-05, 'clf-svm__loss': 'log_loss', 'clf-svm__penalty': 'l2', 'select__k': 3000, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}\n",
            "Best score:  0.9707070707070707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Model Select"
      ],
      "metadata": {
        "id": "TdsaV55y7qYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_test['clean_text'] = final_test['body'].apply(clean_text)"
      ],
      "metadata": {
        "id": "Db4E7fm9f70K"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_X_test = final_test['clean_text']"
      ],
      "metadata": {
        "id": "_zKPW_OdxvVo"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_search.best_params_\n",
        "final_test['clean_text'] = final_test['body'].apply(clean_text)\n",
        "final_X_test = final_test['clean_text']\n",
        "\n",
        "text_model_svm_final = Pipeline([\n",
        "    ('vect', CountVectorizer(ngram_range=best_params['vect__ngram_range'])),\n",
        "    ('tfidf', TfidfTransformer(use_idf=best_params['tfidf__use_idf'])),\n",
        "    ('normalize', Normalizer()),\n",
        "    ('select', SelectKBest(k=best_params['select__k'])),\n",
        "    ('clf-svm', SGDClassifier(alpha=best_params['clf-svm__alpha'], \n",
        "                              loss=best_params['clf-svm__loss'], \n",
        "                              penalty=best_params['clf-svm__penalty']))\n",
        "])\n",
        "text_model_svm_final.fit(X, y)\n",
        "\n",
        "# Evaluate the final model on the test dataset\n",
        "y_pred = text_model_svm_final.predict(final_X_test)\n",
        "y_pred\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvF3HLbaHRGi",
        "outputId": "10f7aa7d-68bf-4144-886b-017cc618c945"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
              "       2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 4, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n",
              "       3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4,\n",
              "       4, 4, 4, 4, 4, 4, 1, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 1, 4, 3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reverse dictionary to map predicted values to original labels\n",
        "reverse_dict = {1: 'Ford', 2: 'Musk', 3: 'Obama', 4: 'Trump'}\n",
        "\n",
        "# Map predicted values to original labels\n",
        "predicted_labels = [reverse_dict[pred] for pred in y_pred]\n",
        "\n",
        "# Create a new DataFrame with the id and subreddit columns\n",
        "final_predict2_0306 = pd.DataFrame({'id': range(len(predicted_labels)), 'subreddit': predicted_labels})\n",
        "final_predict2_0306.to_csv('final_predict666666666666666.csv', index=False)"
      ],
      "metadata": {
        "id": "8NBV9vqEH5Y7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpXbFsKVLQlW",
        "outputId": "43c8a447-ba4c-4fa8-8be4-0784605b19c6"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "279"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}